---
title: "Nonconvex Root PCP for paper"
author: "Rachel Tao"
date: "2/20/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---


## 1. Load packages & set working environment

```{r setup, include=FALSE}
# install pcpr from https://github.com/Columbia-PRIME/pcpr
install.packages("insert your path here", repos = NULL, type = "source")

# install PCPHelpers from https://github.com/Columbia-PRIME/PCPhelpers
install.packages("insert your path here", repos = NULL, type = "source")

# load libraries
library(pcpr) 
library(PCPhelpers) 
library(Matrix)
library(tidyverse)
library(NMF) # this biobase package which needs special downloading.See https://rdrr.io/bioc/Biobase/
library(janitor)
library(ggcorrplot)
library(ggfortify)  
library(gridExtra)
library(knitr)
library(patchwork)
library(GGally)
library(grDevices)
library(gplots) 
library(foreach)
library(doSNOW)
library(tictoc)
library(lubridate)
library(splines)
library(pals)

# load prep_data function to read NYC data in
source(here::here("eda-latest-nyc-data.R"))

# choose color palette large enough for visuallizations needed later
Color.list <- glasbey(28)

# other theme settings
knitr::opts_chunk$set(
	fig.asp = 0.6,
	fig.width = 6,
	message = FALSE,
	warning = FALSE,
	out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis")

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

## 2. Load data

PM2.5 components data and total PM2.5 mass concentrations. 

```{r}

# prep data
start_date <- as.Date("2007-01-01") # set start date
end_date <- as.Date("2015-12-31") # set end date
nyc <- prep_data(years = c(start_date, end_date)) # list; different versions (scaled, not scaled, etc.)

pm25_tot <- read_csv("./data/pm2.5_total_components_daily_nyc_avg.csv") %>% 
  rename(
    date = date_local,
    pm25 = pm2.5_total
  ) %>% 
  select(date, pm25)

nyc_raw <- cbind(nyc$M.dates, nyc$M) %>% 
  as_tibble() %>% 
  rename(date = 'nyc$M.dates') %>% 
  left_join(pm25_tot, by = "date") %>% 
  relocate(date, pm25)

# write_csv(nyc_raw, "./data/nyc_raw.csv")

```

## 3. Correlation Matrix - Figure S3

Correlation between PM2.5 components

Yanelli:

Line 136: maybe changed to label_alpha = FALSE because the labels are difficult to read when they are transparent 

```{r}
# correlation matrix
nyc$M %>%
  rename(
    Al = "aluminum",
    "NH4" = "ammonium ion",
    As = "arsenic",
    Ba = "barium",
    Br = "bromine",
    Cd = "cadmium",
    Ca = "calcium",
    Cl = "chlorine",
    Cr = "chromium",
    Cu = "copper",
    EC = "elemental carbon",
    Fe = "iron",
    Pb = "lead",
    Mg = "magnesium",
    Mn = "manganese",
    Ni = "nickel",
    OC = "organic carbon",
    K = "potassium ion",
    Se = "selenium",
    Si = "silicon",
    Na = "sodium",
    S = "sulfur",
    Ti = "titanium",
    "NO3" = "total nitrate",
    V = "vanadium",
    Zn = "zinc"
  ) %>% 
  ggcorr(method = c("pairwise.complete.obs", "pearson"), limits = FALSE,
       label = TRUE, label_size = 3, label_alpha = TRUE,
       hjust = 1, size = 5, color = "grey50", layout.exp = 4)

```


## 4. Cross validation to select hyperparameters r, lambda, and mu


line 204-207: Should we add to the manuscript a very general summary of the cross validations process? I also don't think we mention in the manuscript the metrics we are using to identify the "optimal values" (aka relative error of the Frobenius norm, sparsity, etc.).
158: Should we mention in the manuscript that we use scaled data for the tuning?
266: I'm curious about how did you decide on the final values of the parameters. I know this is a bit of a subjective process and just so I know how to approach this in a future analysis

```{hyperparameter selection r}
#----------------------------#
#### 1. TEST MATRIX ####
#----------------------------#

# corrupt data
# here, we replace a random 15% of the observations with missing values to create a 
# training set of the remaining 85% of the observations. The 15% that were 
# corrupted (received NA in the training set) will become the test set.
# This code generates a list that inside contains 2 lists: $cor.mat (corrupted matrix) 
# and $cor.mask (1=corrupted values, 0=non-corrupted values)
nyc$M_tilde <- corrupt_mat_randomly(nyc$M.scaled, 1, perc_b = .15) 


# R doesn't like multiplying NA values, so we'll impute NA in the scaled original data as 0
# so they don't affect the calculation of the Frobenius norm
nyc$M_tilde$M_copy <- nyc$M.scaled # make a copy of the scaled data
nyc$M_tilde$M_copy[is.na(nyc$M_tilde$M_copy)] <- 0 # substitute na's with zero

# Get the frobenius norm of the original (scaled) data matrix's test set now
# so we can use it to calculate the relative error in the grid search later.
# we can calculate this norm now because this will be a constant throughout the whole gridsearch,
# i.e. it doesn't depend on the parameter settings, just the original data matrix and
# choice of test set / corruption. therefore, computationally it makes sense to calculate
# it once instead of each iteration of the gridsearch. 
# we use the copy of the original (scaled) data matrix
nyc$M_norm <- norm(nyc$M_tilde$cor.mask * nyc$M_tilde$M_copy, "F")

# Now that we have set up the training/testing set, we need to do a gridsearch to find
# optimal values for our hyperparameters (rank, lambda, mu).
# To do this, we will repeat steps 2-5 with a different range of values until
# we find the optimal combination of values, based on relative error.
#----------------------------#
#### 2. SET UP GRIDSEARCH ####
#----------------------------#
# define the search space
ranks <- 1:10
lambdas <- c(0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, .5, 1, 2)
mus <- c(0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, .5, 1, 2)
# To find the optimal values, we adjusted the grid as needed after viewing the
# results until we found the optimal values for rank, lambda, and mu

grid <- expand.grid(r = ranks, lambda = lambdas, mu = mus) 
# length(ranks)*length(lambdas)-many rows long

# get default parameters so we have a default mu. 
default <- get_pcp_defaults(nyc$M.scaled) # a list containing $lambda and $mu
# these are the theoretically optimal values for lambda and mu that would have 
# been used in other applications of PCP. These are only used for comparison here.

# set up parallel programming
cores <- ceiling(parallel::detectCores() / 2) # optimal number of cores to use is half (rounded up) of the available ones on your computer.
cl <- snow::makeCluster(cores)
doSNOW::registerDoSNOW(cl)

# set up progress bar for parallel (or sequential) programming
pb <- txtProgressBar(min = 0, max = nrow(grid), width = 50, style = 3)
progress <- function(p) setTxtProgressBar(pb, p)
opts <- list(progress = progress)

#-------------------------#
#### 3. RUN GRIDSEARCH ####
#-------------------------#
tic()
grid_search <- foreach(
	i = iterators::icount(nrow(grid)), # i will go from 1 to nrow(grid)
	.options.snow = opts, # the progress bar being sent to each core
	.packages = c("pcpr", "PCPhelpers"), # the packages being sent to each core
	.combine = rbind, # how we combine the last statement of the foreach loop
	.inorder = FALSE # this speeds up the parallel programming
) %dopar% {

# run PCP for the current parameter setting, using the corrupted matrix (test set)
# PCP will also estimate the values for the corrupted observations
pcp <- root_pcp_noncvx_nonnegL_na(nyc$M_tilde$cor.mat, mu = grid$mu[i], lambda = grid$lambda[i], r = grid$r[i])

# calculate the relative error incurred by the parameters.
# Here the metric being used for relative error is the frobenius norm
# of only the test set that pcp came up with compared to the original test set's values
# in the original data. Hence the element-wise multiplication by cor.mask (a matrix of 0's and 1's,
# where 1's denote that element is in the test set).
relerr <- norm(nyc$M_tilde$cor.mask * (nyc$M_tilde$M_copy - pcp$L), "F") / nyc$M_norm
	
# calculate other important metrics like the rank of the output L matrix,
# the sparsity of the output S matrix, and the iterations the run took.
lrank <- Matrix::rankMatrix(pcp$L, tol = 1e-04)
ssparsity <- sparsity(pcp$S, tol = 1e-04)
its <- pcp$final_iter

# uncomment if running sequentially:
#progress(i) # this line should be commented out when running in parallel

# last statement that gets combined with rbind (this step takes 1+ hrs):
data.frame(
		lambda = grid$lambda[i], 
		r = grid$r[i], 
		mu = grid$mu[i], 
		rel_err = relerr,
		L.rank = lrank,
		S.sparsity = ssparsity,
		iterations = its
	)
}
toc()

#-------------------#
#### 4. CLEAN UP ####
#-------------------#
# end the parallel programming -- this is to make you computer happy
snow::stopCluster(cl)

# close the progress bar
close(pb)

#-----------------------#
#### 5. VIEW RESULTS ####
#-----------------------#
print(grid_search %>% arrange(r, lambda, mu, rel_err))

grid_search %>%
  arrange(rel_err) %>%
  view()

#grid_search %>% arrange(rel_err) %>% write_csv( "./rachel_experiments/root_ncnvx_grid_search_l0.00008_m0.005.csv")

```

## 5. Run PCP using selected values for lambda, mu, and r

277: It is a bit unclear hoow did we end up with these values for the parameters
278: how did you determine the tol value? 
279-280: i think it would be really helpful to add comments on what are we doing in these lines of code

```{r}
# this takes a few seconds:
pcp <- root_pcp_noncvx_nonnegL_na(nyc$M.scaled, lambda = 0.00008, mu = 0.005, 7, verbose = TRUE)

L1 <- Matrix::rankMatrix(pcp$L, tol = 1e-04)
S1 <- sparsity(pcp$S, tol = 1e-04)

```

## 6. Run NMF on low rank matrix

Helpful sources for NMF: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-367

287-330: the comments through out this code chunk are so helpful :) 

```{r}

# run NMF using different algorithms to find the one we want to use (takes a few min):
res.multi.method <- nmf(
  as.matrix(pcp$L), # L mat
  rank = 7, # rank for L mat
  method = list("brunet", "KL", "lee", "Frobenius", "offset", "nsNMF"), # algos we want to try
  nrun = 100, # runs per algo
  seed = 123456, # global seed
  .options = 'tvp4') # options to parallelize etc. (t=track, v=verbose, p=parallel, number to specify cores #)

# we will exclude ls-nmf because we don't have an uncertainties matrix,
# so it should default to a different, more standard nmf algorithm 
# (looks like Frobenius based on look with just 5 runs per method)

# plot residual error for each method
plot(res.multi.method)
# since offset performed the best, we will proceed with offset

# run nmf of low rank matrix
pcp_nmf <- nmf(pcp$L, rank = 7, method = "offset", nrun = 30, seed = 123456)

# store loadings as tibble
nmf_loadings <- coef(pcp_nmf) %>% 
  as_tibble() %>% 
  mutate(factor = row_number()) %>% 
  pivot_longer(
    cols = aluminum:zinc,
    names_to = "Chemicals",
    values_to = "Loadings")

# write_csv(nmf_loadings, "./data/pcp_nmf_loadings_offset.csv")

# compute nmf scores to use for subsequent health analysis
nmf_scores <- basis(pcp_nmf) %>% 
  as.data.frame() %>% 
  cbind(nyc$M.dates) %>% 
  rename(date = "nyc$M.dates") %>% 
  select(date, V1, V2, V3, V4, V5, V6, V7) %>% 
  as_tibble()

# write_csv(nmf_scores, "./data/pcp_nmf_scores_offset.csv")

```

## 7. Figure 1: loadings figure

```{r}

pcp_nmf_loadings <- read_csv("./data/pcp_nmf_loadings_offset.csv") %>% 
  rename("chemicals" = "Chemicals", "loadings" = "Loadings") %>% 
  mutate(
    factor = as_factor(factor),
    factor = fct_recode(factor,
                        "Crustal Dust" = "1",
                        "Salt" = "2",
                        "Traffic" = "3",
                        "Regional" = "4",
                        "Cadmium" = "5",
                        "Chromium" = "6",
                        "Barium" = "7"
                        ),
    chemicals = as_factor(chemicals),
    chemicals = fct_recode(chemicals,
                           Al = "aluminum",
                           "NH4" = "ammonium ion",
                           As = "arsenic",
                           Ba = "barium",
                           Br = "bromine",
                           Cd = "cadmium",
                           Ca = "calcium",
                           Cl = "chlorine",
                           Cr = "chromium",
                           Cu = "copper",
                           EC = "elemental carbon",
                           Fe = "iron",
                           Pb = "lead",
                           Mg = "magnesium",
                           Mn = "manganese",
                           Ni = "nickel",
                           OC = "organic carbon",
                           K = "potassium ion",
                           Se = "selenium",
                           Si = "silicon",
                           Na = "sodium",
                           S = "sulfur",
                           Ti = "titanium",
                           "NO3" = "total nitrate",
                           V = "vanadium",
                           Zn = "zinc"
                           )
  )

# loadings figure
pcp_nmf_loadings %>% 
  ggplot(aes(x = chemicals, y = loadings)) + 
  geom_col() +
  geom_hline(yintercept = 0, size = 0.2) + 
  facet_wrap(. ~ factor, ncol = 1, scales = "free") +
  theme_bw() +
  ylim(0, 0.45) +
  xlab("") +
  ylab("PCP-NMF Loadings") +
  theme(
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 12),
    strip.text = element_text(size = 16),
    axis.title.y = element_text(size = 16)
  )

```


## 8. Create Sparse Matrix
```{r}
# label each sparse value with its date using nyc dataseet
sparse_df <- pcp$S %>% 
  as_tibble() %>% 
  cbind(nyc_raw$date) %>% 
  rename(date = "nyc_raw$date") %>% 
  relocate(date) %>% 
  as_tibble()

# write_csv(sparse_df, "./data/root_ncnvx_sparse_mat.csv")
```


## 9. Figure S11: sparse matrix heatmap

428: Here we are getting rid off of a couple of observations (from 1108 to 1095). Are those not important? should we mention this in the manuscript figure legend? 
```{r}

# create date tibble (this makes the heatmap easier to read)
dates <- seq(as.Date('2007-01-03'), as.Date('2015-12-31'), by = 3) %>% 
  as_tibble() %>% 
  rename(date = 'value')

# bind sparse matrix to new date tibble
all_dates <- left_join(dates, sparse_df, by = "date")

# pivot longer
sparse <- all_dates %>% 
  pivot_longer(aluminum:zinc,
               names_to = "element",
               values_to = "score")

# create sparse matrix heatmap - Figure S11
sparse_heatmap_all <- 
  sparse %>% 
  mutate(
    element = fct_recode(element,
    Al = "aluminum",
    "NH4" = "ammonium ion",
    As = "arsenic",
    Ba = "barium",
    Br = "bromine",
    Cd = "cadmium",
    Ca = "calcium",
    Cl = "chlorine",
    Cr = "chromium",
    Cu = "copper",
    EC = "elemental carbon",
    Fe = "iron",
    Pb = "lead",
    Mg = "magnesium",
    Mn = "manganese",
    Ni = "nickel",
    OC = "organic carbon",
    K = "potassium ion",
    Se = "selenium",
    Si = "silicon",
    Na = "sodium",
    S = "sulfur",
    Ti = "titanium",
    "NO3" = "total nitrate",
    V = "vanadium",
    Zn = "zinc"
  )) %>% 
  ggplot(aes(date, element, fill = score)) +
  geom_tile() +
  xlab("") +
  ylab("") +
  scale_x_date(date_breaks = "years", date_labels = format("%b-%Y")) +
  scale_fill_gradient(low = "lightgrey", high = "black", na.value = "white") +
  theme_classic() +
  theme(
    axis.text = element_text(size = 16),
    axis.text.x = element_text(angle = 315)
  )

sparse_heatmap_all

```

